备注：题目是靠记忆的，有些题只描述了题目大致意思，没有详细表述完整的题目内容。题目顺序非考试顺序，在我考完之后同事去考遇见的题目和我以下列出的是完全一致的。再者，考证并非最终目的，更多的是在以后Kubernetes使用过程中的学习交流。
第一题：
1. 创建clusterrole,并且对该clusterrole只绑定对Deployment，Daemonset,Statefulset的创建权限
2. 在指定namespace创建一个serviceaccount，并且将上一步创建clusterrole和该serviceaccount绑定
参考链接
#1. 创建对应的ClusterRole,并绑定对应的权限
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  namespace: test01 
  name: test01cr
rules:
- apiGroups: ["apps"] # "" indicates the core API group
  resources: ["daemonsets", "deployments", "statefulsets"]
  verbs: ["created"]
​
# 2.创建ServiceAccount
$ kubectl create sa test01sa -n test01
# 3.将ServiceAccount与ClusterRole进行绑定
$ kubectl create rolebinding test01rb --clusterrole=test01cr  --serviceaccount=test01:test01sa --namespace=test01
第二题：
对指定etcd集群进行备份和还原,考试时会给定endpoints, 根证书，证书签名，私钥。
备份：要求备份到指定路径及指定文件名
$ etcdctl --endpoints="https://127.0.0.1:2379" --cacert=ca.crt --key=server.key --cert=server.crt snapshot save /var/data/test.db
还原： 要求使用指定文件进行还原
$ etcdctl --endpoints="https://127.0.0.1:2379" --cacert=ca.crt --key=server.key --cert=server.crt snapshot restore /var/data/test01.db
第三题：
升级集群，将集群中master所有组件从v1.18.8升级到1.19.0(controller,apiserver,scheduler,kubelet,kubectl)
# 1. 将节点标记为不可调度状态
$ kubectl cordon <ndoeName>
# 2. 驱逐节点上的容器
$ kubectl drain <nodeName> --delete-local-data --ignore-daemonsets --force
# 3. ssh 到需要升级的节点上,执行后续操作
# 4. 升级几个二进制组件
$ apt-get install kubeadm=1.19.0-00 kubelet=1.19.0-00 kubectl=1.19.0-00
# 5. 重启kubelet服务
$ systemctl restart kubelet
# 6. 升级集群其他组件
$ kubeadm upgrade apply v1.19.0
第四题：
创建Ingress，将指定的Service的指定端口暴露出来
参考链接
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: ping
  namespace: ing-internal
  annotations:
    nginx.ingress.kubernetes.io/rewrite-target: /
spec:
  rules:
  - http:
      paths:
      - path: /hi
        pathType: Prefix
        backend:
          service:
            name: hi
            port:
              number: 5678
第五题：
创建一个拥有多个（2-4）container容器的Pod:nginx+redis
apiVersion: v1
kind: Pod
metadata:
  name: test05
  namespace: test05
spec:
  containers:
  - image: nginx
    name: nginx
  - image: redis
    name: redis
第六题：
对集群中的PV按照大小顺序排序显示，并将结果写道指定文件
$ kubectl get pv --sort-by=.spec.capacity.storage > /opt/KUCC0010/my_volumes
第七题：
将一个Deployment的副本数量从1个副本扩至3个
$ kubectl scale deployment test07 --replicas=3
第八题：
创建一个Networkpolicy,名称为allow-port-from-namespace, 允许现有namespace internal中得Pod连接同一个namespace中其他Pods的9999端口。
不允许对没有监听8080端口的Pods进行访问
不允许不来自namespace internal的Pods的访问
kind: NetworkPolicy
apiVersion: networking.k8s.io/v1
metadata:
  name: allow-port-from-namespace
  namespace: internal
spec:
  podSelector:
    matchLabels: {}
  ingress:
  - from:
    - podSelector: {}
    ports:
    - port: 9999
第九题：
集群中存在一个Pod，并且该Pod中的容器会将log输出到指定文件。修改Pod配置，将Pod的日志输出到控制台。
其实就是给Pod添加一个sidecar，使用emptyDir, 共享日志存储目录,然后不断读取指定文件，输出到控制台
apiVersion: v1
kind: Pod
metadata:
  name: counter
spec:
  containers:
  - name: count
    image: busybox
    args:
    - /bin/sh
    - -c
    - >
      i=0;
      while true;
      do
        echo "$(date) INFO $i" >> /var/log/info.log;
        i=$((i+1));
        sleep 1;
      done
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  - name: consumer
    image: busybox
    args: [/bin/sh, -c, 'tail -n+1 -f /var/log/info.log']
    volumeMounts:
    - name: varlog
      mountPath: /var/log
  volumes:
  - name: varlog
    emptyDir: {}
第十题：
查询集群中指定Pod的log日志，将带有Error的行输出到指定文件
$ kubectl logs test10 | grep Error > /var/data/test10.log
第十一题：
1.创建一个Deployment，2.更新镜像版本，3.回滚
root@youshin:~# kubectl create deployment test11 --image=nginx
deployment.apps/test11 created
root@youshin:~# kubectl set image deployment/test11 nginx=nginx:1.18 --record 
deployment.apps/test11 image updated
root@youshin:~# kubectl rollout history deployment test11 
deployment.apps/test11 
REVISION  CHANGE-CAUSE
1         <none>
2         kubectl set image deployment/test11 nginx=nginx:1.18 --record=true
root@youshin:~# kubectl rollout undo --to-revision=1 deployment test11 
deployment.apps/test11 rolled back
第十二题：
集群有一个节点notready，找出问题，并解决。并保证机器重启后不会再出现此问题
$ kubectl get nodes #找到NotReady的节点
$ ssh <nodeName> # 连接到NotReady节点
$ systemctl status kubelet # 发现kubelet Not Running
$ systemctl start kubelet #启动kubelet服务
$ systemctl enable kubelet #保证重启后不会再出现此类问题
第十三题：
创建一个PV，使用hostPath存储，大小1G，ReadWriteOnce
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv0003
spec:
  capacity:
    storage: 1Gi
  volumeMode: Filesystem
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Recycle
  storageClassName: slow
  hostPath:
    path: /opt/data/pv003
第十四题:
1.使用指定storageclass创建一个pvc，大小为10M
2. 将这个nginx容器的/var/nginx/html目录使用该pvc挂在出来
3. 将这个pvc的大小从10M更新成70M
# 大像这个文件这么配置,创建之后edit以下pvc,将大小更新为70Mi
apiVersion: v1
kind: PersistentVolumeClaim
metadata:
  name: myclaim
spec:
  accessModes:
    - ReadWriteOnce
  volumeMode: Filesystem
  resources:
    requests:
      storage: 10Mi
  storageClassName: test14
---
apiVersion: v1
kind: Pod
metadata:
  name: mypod
spec:
  containers:
    - name: nginx
      image: nginx
      volumeMounts:
      - mountPath: "/var/www/html"
        name: mypd
  volumes:
    - name: mypd
      persistentVolumeClaim:
        claimName: myclaim
第十五题
将集群中一个Deployment服务暴露出来,(是一个nginx，使用kubectl expose命令暴露即可)
$ kubectl expose deployment test15 --port 80 --target-port=80 --type="NodePort"
第十六题
查询集群中节点，找出可以调度节点的数量，（其实就是被标记为不可调度和打了污点的节点之外的节点数量 ），然后将数量写到指定文件
# 笨方法,数
# 1. 查询集群Ready节点数量
$ kubectl get node | grep -i ready
# 2. 判断节点有误不可调度污点
$ kubectl describe nodes <nodeName>  |  grep -i taints | grep -i nodeSchedule
第十七题
找集群中带有指定label的Pod中占用资源最高的，并将它的名字写入指定的文件
$ kubectl top pod -l key=value #筛选指定带有指定label的Pod资源占用情况